{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO44PZmXM/rxClW0ynPgAc/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ezzywd78/MS_AI_Engineer_Course/blob/main/Mercedes_Benz_Greener_Manufacturing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsVI2sf7L_cr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "258e9b9f"
      },
      "source": [
        "# Task\n",
        "Load and preprocess data from \"train.zip\" and \"test.zip\" including unzipping, initial exploration, cleaning zero-variance columns, applying label encoding, and performing dimensionality reduction. Then, train an XGBoost regression model on the processed training data to predict the 'y' column, make predictions on the preprocessed test set, and summarize the entire process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceef06d7"
      },
      "source": [
        "## Unzip and Load Data\n",
        "\n",
        "### Subtask:\n",
        "Unzip the provided 'train.zip' and 'test.zip' files and load the data into pandas DataFrames for training and testing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acb0e614"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to import the necessary libraries, create directories for the extracted data, and then unzip the provided 'train.zip' and 'test.zip' files into their respective directories.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94bffafa",
        "outputId": "689f95a4-6424-4352-aa01-fe3839e6eff7"
      },
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Create 'train_data' directory and extract train.zip\n",
        "os.makedirs('train_data', exist_ok=True)\n",
        "with zipfile.ZipFile('/content/train.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('train_data')\n",
        "print(\"train.zip extracted to train_data/\")\n",
        "\n",
        "# Create 'test_data' directory and extract test.zip\n",
        "os.makedirs('test_data', exist_ok=True)\n",
        "with zipfile.ZipFile('/content/test.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('test_data')\n",
        "print(\"test.zip extracted to test_data/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train.zip extracted to train_data/\n",
            "test.zip extracted to test_data/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f2d2b90"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the files are unzipped, the next step is to load the data from 'train.csv' and 'test.csv' into pandas DataFrames, and then display the first 5 rows of each to verify the loading process.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49cb0085",
        "outputId": "2679e256-8201-45a1-9578-f3713e3e70b6"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the training data\n",
        "train_df = pd.read_csv('train_data/train.csv')\n",
        "print(\"Train DataFrame loaded. First 5 rows:\")\n",
        "print(train_df.head())\n",
        "\n",
        "# Load the test data\n",
        "test_df = pd.read_csv('test_data/test.csv')\n",
        "print(\"\\nTest DataFrame loaded. First 5 rows:\")\n",
        "print(test_df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train DataFrame loaded. First 5 rows:\n",
            "   ID       y  X0 X1  X2 X3 X4 X5 X6 X8  ...  X375  X376  X377  X378  X379  \\\n",
            "0   0  130.81   k  v  at  a  d  u  j  o  ...     0     0     1     0     0   \n",
            "1   6   88.53   k  t  av  e  d  y  l  o  ...     1     0     0     0     0   \n",
            "2   7   76.26  az  w   n  c  d  x  j  x  ...     0     0     0     0     0   \n",
            "3   9   80.62  az  t   n  f  d  x  l  e  ...     0     0     0     0     0   \n",
            "4  13   78.02  az  v   n  f  d  h  d  n  ...     0     0     0     0     0   \n",
            "\n",
            "   X380  X382  X383  X384  X385  \n",
            "0     0     0     0     0     0  \n",
            "1     0     0     0     0     0  \n",
            "2     0     1     0     0     0  \n",
            "3     0     0     0     0     0  \n",
            "4     0     0     0     0     0  \n",
            "\n",
            "[5 rows x 378 columns]\n",
            "\n",
            "Test DataFrame loaded. First 5 rows:\n",
            "   ID  X0 X1  X2 X3 X4 X5 X6 X8  X10  ...  X375  X376  X377  X378  X379  X380  \\\n",
            "0   1  az  v   n  f  d  t  a  w    0  ...     0     0     0     1     0     0   \n",
            "1   2   t  b  ai  a  d  b  g  y    0  ...     0     0     1     0     0     0   \n",
            "2   3  az  v  as  f  d  a  j  j    0  ...     0     0     0     1     0     0   \n",
            "3   4  az  l   n  f  d  z  l  n    0  ...     0     0     0     1     0     0   \n",
            "4   5   w  s  as  c  d  y  i  m    0  ...     1     0     0     0     0     0   \n",
            "\n",
            "   X382  X383  X384  X385  \n",
            "0     0     0     0     0  \n",
            "1     0     0     0     0  \n",
            "2     0     0     0     0  \n",
            "3     0     0     0     0  \n",
            "4     0     0     0     0  \n",
            "\n",
            "[5 rows x 377 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d6d3ca9"
      },
      "source": [
        "## Initial Data Exploration and Cleaning\n",
        "\n",
        "### Subtask:\n",
        "Perform an initial inspection of both the training and test datasets. This includes checking for null values, examining unique values for categorical features, and understanding the data types and distributions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bc145d8"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step in data exploration is to check for missing values, understand data types, and get basic descriptive statistics for numerical columns. This can be achieved for both train_df and test_df using `.isnull().sum()`, `.info()`, and `.describe()` methods.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb8e5552",
        "outputId": "c60a0fe8-6ec0-4b6d-c52a-14eee7e00ce6"
      },
      "source": [
        "print(\"--- Training Data Info ---\")\n",
        "train_df.info()\n",
        "print(\"\\n--- Missing Values in Training Data ---\")\n",
        "print(train_df.isnull().sum()[train_df.isnull().sum() > 0])\n",
        "print(\"\\n--- Descriptive Statistics for Training Data ---\")\n",
        "print(train_df.describe())\n",
        "\n",
        "print(\"\\n\\n--- Test Data Info ---\")\n",
        "test_df.info()\n",
        "print(\"\\n--- Missing Values in Test Data ---\")\n",
        "print(test_df.isnull().sum()[test_df.isnull().sum() > 0])\n",
        "print(\"\\n--- Descriptive Statistics for Test Data ---\")\n",
        "print(test_df.describe())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Training Data Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4209 entries, 0 to 4208\n",
            "Columns: 378 entries, ID to X385\n",
            "dtypes: float64(1), int64(369), object(8)\n",
            "memory usage: 12.1+ MB\n",
            "\n",
            "--- Missing Values in Training Data ---\n",
            "Series([], dtype: int64)\n",
            "\n",
            "--- Descriptive Statistics for Training Data ---\n",
            "                ID            y          X10     X11          X12  \\\n",
            "count  4209.000000  4209.000000  4209.000000  4209.0  4209.000000   \n",
            "mean   4205.960798   100.669318     0.013305     0.0     0.075077   \n",
            "std    2437.608688    12.679381     0.114590     0.0     0.263547   \n",
            "min       0.000000    72.110000     0.000000     0.0     0.000000   \n",
            "25%    2095.000000    90.820000     0.000000     0.0     0.000000   \n",
            "50%    4220.000000    99.150000     0.000000     0.0     0.000000   \n",
            "75%    6314.000000   109.010000     0.000000     0.0     0.000000   \n",
            "max    8417.000000   265.320000     1.000000     0.0     1.000000   \n",
            "\n",
            "               X13          X14          X15          X16          X17  ...  \\\n",
            "count  4209.000000  4209.000000  4209.000000  4209.000000  4209.000000  ...   \n",
            "mean      0.057971     0.428130     0.000475     0.002613     0.007603  ...   \n",
            "std       0.233716     0.494867     0.021796     0.051061     0.086872  ...   \n",
            "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
            "25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
            "50%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
            "75%       0.000000     1.000000     0.000000     0.000000     0.000000  ...   \n",
            "max       1.000000     1.000000     1.000000     1.000000     1.000000  ...   \n",
            "\n",
            "              X375         X376         X377         X378         X379  \\\n",
            "count  4209.000000  4209.000000  4209.000000  4209.000000  4209.000000   \n",
            "mean      0.318841     0.057258     0.314802     0.020670     0.009503   \n",
            "std       0.466082     0.232363     0.464492     0.142294     0.097033   \n",
            "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
            "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
            "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
            "75%       1.000000     0.000000     1.000000     0.000000     0.000000   \n",
            "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
            "\n",
            "              X380         X382         X383         X384         X385  \n",
            "count  4209.000000  4209.000000  4209.000000  4209.000000  4209.000000  \n",
            "mean      0.008078     0.007603     0.001663     0.000475     0.001426  \n",
            "std       0.089524     0.086872     0.040752     0.021796     0.037734  \n",
            "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
            "25%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
            "50%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
            "75%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
            "max       1.000000     1.000000     1.000000     1.000000     1.000000  \n",
            "\n",
            "[8 rows x 370 columns]\n",
            "\n",
            "\n",
            "--- Test Data Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4209 entries, 0 to 4208\n",
            "Columns: 377 entries, ID to X385\n",
            "dtypes: int64(369), object(8)\n",
            "memory usage: 12.1+ MB\n",
            "\n",
            "--- Missing Values in Test Data ---\n",
            "Series([], dtype: int64)\n",
            "\n",
            "--- Descriptive Statistics for Test Data ---\n",
            "                ID          X10          X11          X12          X13  \\\n",
            "count  4209.000000  4209.000000  4209.000000  4209.000000  4209.000000   \n",
            "mean   4211.039202     0.019007     0.000238     0.074364     0.061060   \n",
            "std    2423.078926     0.136565     0.015414     0.262394     0.239468   \n",
            "min       1.000000     0.000000     0.000000     0.000000     0.000000   \n",
            "25%    2115.000000     0.000000     0.000000     0.000000     0.000000   \n",
            "50%    4202.000000     0.000000     0.000000     0.000000     0.000000   \n",
            "75%    6310.000000     0.000000     0.000000     0.000000     0.000000   \n",
            "max    8416.000000     1.000000     1.000000     1.000000     1.000000   \n",
            "\n",
            "               X14          X15          X16          X17          X18  ...  \\\n",
            "count  4209.000000  4209.000000  4209.000000  4209.000000  4209.000000  ...   \n",
            "mean      0.427893     0.000713     0.002613     0.008791     0.010216  ...   \n",
            "std       0.494832     0.026691     0.051061     0.093357     0.100570  ...   \n",
            "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
            "25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
            "50%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
            "75%       1.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
            "max       1.000000     1.000000     1.000000     1.000000     1.000000  ...   \n",
            "\n",
            "              X375         X376         X377         X378         X379  \\\n",
            "count  4209.000000  4209.000000  4209.000000  4209.000000  4209.000000   \n",
            "mean      0.325968     0.049656     0.311951     0.019244     0.011879   \n",
            "std       0.468791     0.217258     0.463345     0.137399     0.108356   \n",
            "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
            "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
            "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
            "75%       1.000000     0.000000     1.000000     0.000000     0.000000   \n",
            "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
            "\n",
            "              X380         X382         X383         X384         X385  \n",
            "count  4209.000000  4209.000000  4209.000000  4209.000000  4209.000000  \n",
            "mean      0.008078     0.008791     0.000475     0.000713     0.001663  \n",
            "std       0.089524     0.093357     0.021796     0.026691     0.040752  \n",
            "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
            "25%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
            "50%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
            "75%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
            "max       1.000000     1.000000     1.000000     1.000000     1.000000  \n",
            "\n",
            "[8 rows x 369 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6c5cd2d"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that we have reviewed the basic information and descriptive statistics, the next step is to examine the categorical features as specified in the instructions. This involves identifying object-type columns, counting their unique values, and showing value counts for a few to understand their distribution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7372288d",
        "outputId": "cd153606-e360-444f-d2f3-8821ee4f44ec"
      },
      "source": [
        "print(\"\\n--- Categorical Features in Training Data ---\")\n",
        "train_object_cols = train_df.select_dtypes(include='object').columns\n",
        "print(f\"Found {len(train_object_cols)} object columns in train_df: {list(train_object_cols)}\")\n",
        "for col in train_object_cols:\n",
        "    print(f\"Column '{col}': {train_df[col].nunique()} unique values\")\n",
        "    if train_df[col].nunique() < 20: # Display value counts for columns with fewer unique values\n",
        "        print(train_df[col].value_counts())\n",
        "    print(\"\\n\")\n",
        "\n",
        "print(\"\\n--- Categorical Features in Test Data ---\")\n",
        "test_object_cols = test_df.select_dtypes(include='object').columns\n",
        "print(f\"Found {len(test_object_cols)} object columns in test_df: {list(test_object_cols)}\")\n",
        "for col in test_object_cols:\n",
        "    print(f\"Column '{col}': {test_df[col].nunique()} unique values\")\n",
        "    if test_df[col].nunique() < 20: # Display value counts for columns with fewer unique values\n",
        "        print(test_df[col].value_counts())\n",
        "    print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Categorical Features in Training Data ---\n",
            "Found 8 object columns in train_df: ['X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X8']\n",
            "Column 'X0': 47 unique values\n",
            "\n",
            "\n",
            "Column 'X1': 27 unique values\n",
            "\n",
            "\n",
            "Column 'X2': 44 unique values\n",
            "\n",
            "\n",
            "Column 'X3': 7 unique values\n",
            "X3\n",
            "c    1942\n",
            "f    1076\n",
            "a     440\n",
            "d     290\n",
            "g     241\n",
            "e     163\n",
            "b      57\n",
            "Name: count, dtype: int64\n",
            "\n",
            "\n",
            "Column 'X4': 4 unique values\n",
            "X4\n",
            "d    4205\n",
            "a       2\n",
            "b       1\n",
            "c       1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "\n",
            "Column 'X5': 29 unique values\n",
            "\n",
            "\n",
            "Column 'X6': 12 unique values\n",
            "X6\n",
            "g    1042\n",
            "j    1039\n",
            "d     625\n",
            "i     488\n",
            "l     478\n",
            "a     206\n",
            "h     190\n",
            "k      43\n",
            "c      38\n",
            "b      28\n",
            "f      20\n",
            "e      12\n",
            "Name: count, dtype: int64\n",
            "\n",
            "\n",
            "Column 'X8': 25 unique values\n",
            "\n",
            "\n",
            "\n",
            "--- Categorical Features in Test Data ---\n",
            "Found 8 object columns in test_df: ['X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X8']\n",
            "Column 'X0': 49 unique values\n",
            "\n",
            "\n",
            "Column 'X1': 27 unique values\n",
            "\n",
            "\n",
            "Column 'X2': 45 unique values\n",
            "\n",
            "\n",
            "Column 'X3': 7 unique values\n",
            "X3\n",
            "c    1900\n",
            "f    1083\n",
            "a     476\n",
            "d     274\n",
            "g     272\n",
            "e     158\n",
            "b      46\n",
            "Name: count, dtype: int64\n",
            "\n",
            "\n",
            "Column 'X4': 4 unique values\n",
            "X4\n",
            "d    4203\n",
            "b       4\n",
            "a       1\n",
            "c       1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "\n",
            "Column 'X5': 32 unique values\n",
            "\n",
            "\n",
            "Column 'X6': 12 unique values\n",
            "X6\n",
            "g    1073\n",
            "j    1002\n",
            "d     589\n",
            "i     490\n",
            "l     473\n",
            "h     218\n",
            "a     196\n",
            "k      67\n",
            "c      40\n",
            "f      25\n",
            "b      19\n",
            "e      17\n",
            "Name: count, dtype: int64\n",
            "\n",
            "\n",
            "Column 'X8': 25 unique values\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b49a74b8"
      },
      "source": [
        "## Remove Zero-Variance Columns\n",
        "\n",
        "### Subtask:\n",
        "Identify and remove columns from both the training and test datasets that have a variance equal to zero, as these columns do not contribute any information.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01e3dac0"
      },
      "source": [
        "**Reasoning**:\n",
        "First, I will identify columns in the training data (excluding 'ID' and 'y') that have zero variance. This involves calculating the variance for each numerical column and filtering those with a variance of 0.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c80813b4",
        "outputId": "45a9ef71-1236-41bf-a714-5882875f6aba"
      },
      "source": [
        "zero_variance_cols_train = [col for col in train_df.columns if col not in ['ID', 'y'] and train_df[col].nunique() == 1]\n",
        "print(f\"Columns with zero variance in train_df (excluding 'ID' and 'y'): {len(zero_variance_cols_train)}\")\n",
        "print(zero_variance_cols_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns with zero variance in train_df (excluding 'ID' and 'y'): 12\n",
            "['X11', 'X93', 'X107', 'X233', 'X235', 'X268', 'X289', 'X290', 'X293', 'X297', 'X330', 'X347']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4fc0523"
      },
      "source": [
        "**Reasoning**:\n",
        "Next, I will identify columns in the test data (excluding 'ID') that have zero variance. This involves checking the number of unique values for each column and filtering those with only one unique value.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70116c18",
        "outputId": "95d68840-85c3-4020-cad9-12abb439935e"
      },
      "source": [
        "zero_variance_cols_test = [col for col in test_df.columns if col != 'ID' and test_df[col].nunique() == 1]\n",
        "print(f\"Columns with zero variance in test_df (excluding 'ID'): {len(zero_variance_cols_test)}\")\n",
        "print(zero_variance_cols_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns with zero variance in test_df (excluding 'ID'): 5\n",
            "['X257', 'X258', 'X295', 'X296', 'X369']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59d85816"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that zero-variance columns for both train and test datasets have been identified separately, I will combine them into a single set to ensure consistent removal across both dataframes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86d54162",
        "outputId": "fdbf853e-5d21-4acb-bb93-aa1bbf04b9a0"
      },
      "source": [
        "all_zero_variance_cols = list(set(zero_variance_cols_train + zero_variance_cols_test))\n",
        "print(f\"Combined zero-variance columns: {len(all_zero_variance_cols)}\")\n",
        "print(all_zero_variance_cols)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined zero-variance columns: 17\n",
            "['X369', 'X93', 'X296', 'X235', 'X268', 'X11', 'X295', 'X290', 'X293', 'X258', 'X330', 'X289', 'X347', 'X297', 'X257', 'X233', 'X107']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1292b6bc"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the common set of zero-variance columns has been identified, I will remove these columns from both the training and test DataFrames to ensure consistency and eliminate non-informative features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95dc93ca",
        "outputId": "6462669b-bec7-440c-a706-0dfaaace0e10"
      },
      "source": [
        "initial_train_cols = train_df.shape[1]\n",
        "initial_test_cols = test_df.shape[1]\n",
        "\n",
        "train_df = train_df.drop(columns=all_zero_variance_cols, errors='ignore')\n",
        "test_df = test_df.drop(columns=all_zero_variance_cols, errors='ignore')\n",
        "\n",
        "removed_train_cols = initial_train_cols - train_df.shape[1]\n",
        "removed_test_cols = initial_test_cols - test_df.shape[1]\n",
        "\n",
        "print(f\"Number of columns removed from train_df: {removed_train_cols}\")\n",
        "print(f\"New shape of train_df: {train_df.shape}\")\n",
        "print(f\"Number of columns removed from test_df: {removed_test_cols}\")\n",
        "print(f\"New shape of test_df: {test_df.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of columns removed from train_df: 17\n",
            "New shape of train_df: (4209, 361)\n",
            "Number of columns removed from test_df: 17\n",
            "New shape of test_df: (4209, 360)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14fedf3c"
      },
      "source": [
        "## Apply Label Encoding\n",
        "\n",
        "### Subtask:\n",
        "Apply Label Encoding to all categorical features in both the training and test datasets to convert them into numerical representations suitable for machine learning models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "559e2069"
      },
      "source": [
        "**Reasoning**:\n",
        "First, I will identify the categorical columns (object type) present in both the training and test datasets. It is crucial to find the intersection of these columns to ensure that encoding is applied consistently to features common in both datasets. I will list these columns to verify they have been correctly identified.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91db1acb",
        "outputId": "c659b125-7ba6-4878-f41b-8fa9ab2f27cd"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Identify categorical columns in train_df\n",
        "train_categorical_cols = train_df.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# Identify categorical columns in test_df\n",
        "test_categorical_cols = test_df.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# Get the common categorical columns present in both train and test datasets\n",
        "common_categorical_cols = list(set(train_categorical_cols) & set(test_categorical_cols))\n",
        "\n",
        "print(f\"Categorical columns in train_df: {train_categorical_cols}\")\n",
        "print(f\"Categorical columns in test_df: {test_categorical_cols}\")\n",
        "print(f\"Common categorical columns for encoding: {common_categorical_cols}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categorical columns in train_df: ['X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X8']\n",
            "Categorical columns in test_df: ['X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X8']\n",
            "Common categorical columns for encoding: ['X0', 'X6', 'X1', 'X2', 'X4', 'X8', 'X5', 'X3']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d25371b"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the common categorical columns have been identified, I will iterate through each of them, create a comprehensive set of unique values from both the training and test datasets for each column, fit a LabelEncoder to these combined values, and then transform the respective columns in both DataFrames. This ensures consistent encoding across the datasets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4d2b9e9",
        "outputId": "b991e76c-db42-481d-eff7-5ec42992a2d8"
      },
      "source": [
        "label_encoders = {}\n",
        "for col in common_categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    # Combine unique values from train and test for comprehensive fitting\n",
        "    combined_values = pd.concat([train_df[col], test_df[col]], axis=0).astype(str).unique()\n",
        "    le.fit(combined_values)\n",
        "\n",
        "    train_df[col] = le.transform(train_df[col].astype(str))\n",
        "    test_df[col] = le.transform(test_df[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "\n",
        "print(\"Label Encoding applied to common categorical columns.\")\n",
        "print(\"First 5 rows of train_df after encoding:\")\n",
        "print(train_df.head())\n",
        "print(\"\\nFirst 5 rows of test_df after encoding:\")\n",
        "print(test_df.head())\n",
        "print(\"\\nData types of encoded columns in train_df:\")\n",
        "print(train_df[common_categorical_cols].dtypes)\n",
        "print(\"\\nData types of encoded columns in test_df:\")\n",
        "print(test_df[common_categorical_cols].dtypes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label Encoding applied to common categorical columns.\n",
            "First 5 rows of train_df after encoding:\n",
            "   ID       y  X0  X1  X2  X3  X4  X5  X6  X8  ...  X375  X376  X377  X378  \\\n",
            "0   0  130.81  37  23  20   0   3  27   9  14  ...     0     0     1     0   \n",
            "1   6   88.53  37  21  22   4   3  31  11  14  ...     1     0     0     0   \n",
            "2   7   76.26  24  24  38   2   3  30   9  23  ...     0     0     0     0   \n",
            "3   9   80.62  24  21  38   5   3  30  11   4  ...     0     0     0     0   \n",
            "4  13   78.02  24  23  38   5   3  14   3  13  ...     0     0     0     0   \n",
            "\n",
            "   X379  X380  X382  X383  X384  X385  \n",
            "0     0     0     0     0     0     0  \n",
            "1     0     0     0     0     0     0  \n",
            "2     0     0     1     0     0     0  \n",
            "3     0     0     0     0     0     0  \n",
            "4     0     0     0     0     0     0  \n",
            "\n",
            "[5 rows x 361 columns]\n",
            "\n",
            "First 5 rows of test_df after encoding:\n",
            "   ID  X0  X1  X2  X3  X4  X5  X6  X8  X10  ...  X375  X376  X377  X378  X379  \\\n",
            "0   1  24  23  38   5   3  26   0  22    0  ...     0     0     0     1     0   \n",
            "1   2  46   3   9   0   3   9   6  24    0  ...     0     0     1     0     0   \n",
            "2   3  24  23  19   5   3   0   9   9    0  ...     0     0     0     1     0   \n",
            "3   4  24  13  38   5   3  32  11  13    0  ...     0     0     0     1     0   \n",
            "4   5  49  20  19   2   3  31   8  12    0  ...     1     0     0     0     0   \n",
            "\n",
            "   X380  X382  X383  X384  X385  \n",
            "0     0     0     0     0     0  \n",
            "1     0     0     0     0     0  \n",
            "2     0     0     0     0     0  \n",
            "3     0     0     0     0     0  \n",
            "4     0     0     0     0     0  \n",
            "\n",
            "[5 rows x 360 columns]\n",
            "\n",
            "Data types of encoded columns in train_df:\n",
            "X0    int64\n",
            "X6    int64\n",
            "X1    int64\n",
            "X2    int64\n",
            "X4    int64\n",
            "X8    int64\n",
            "X5    int64\n",
            "X3    int64\n",
            "dtype: object\n",
            "\n",
            "Data types of encoded columns in test_df:\n",
            "X0    int64\n",
            "X6    int64\n",
            "X1    int64\n",
            "X2    int64\n",
            "X4    int64\n",
            "X8    int64\n",
            "X5    int64\n",
            "X3    int64\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d45ed293"
      },
      "source": [
        "## Perform Dimensionality Reduction\n",
        "\n",
        "### Subtask:\n",
        "Apply a dimensionality reduction technique, such as Principal Component Analysis (PCA), to the processed training and test datasets to reduce the number of features while retaining most of the important information.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e9a15b6"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step in applying PCA is to separate the target variable and identifier columns from the feature sets for both the training and test datasets. This ensures that PCA is applied only to the features, and that 'ID' and 'y' columns are handled appropriately.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47373d9f",
        "outputId": "3766654e-c46a-4415-ccea-58cac0352198"
      },
      "source": [
        "y_train = train_df['y']\n",
        "train_ids = train_df['ID']\n",
        "test_ids = test_df['ID']\n",
        "\n",
        "X_train = train_df.drop(columns=['ID', 'y'])\n",
        "X_test = test_df.drop(columns=['ID'])\n",
        "\n",
        "print(f\"Shape of X_train: {X_train.shape}\")\n",
        "print(f\"Shape of y_train: {y_train.shape}\")\n",
        "print(f\"Shape of X_test: {X_test.shape}\")\n",
        "print(f\"Shape of train_ids: {train_ids.shape}\")\n",
        "print(f\"Shape of test_ids: {test_ids.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train: (4209, 359)\n",
            "Shape of y_train: (4209,)\n",
            "Shape of X_test: (4209, 359)\n",
            "Shape of train_ids: (4209,)\n",
            "Shape of test_ids: (4209,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88df594d"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the data is separated, I will initialize a StandardScaler and fit it to the training features (`X_train`) to learn the scaling parameters. Then, I will transform both the training (`X_train`) and test (`X_test`) features using this fitted scaler to standardize them, which is a common prerequisite for PCA.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4776258b",
        "outputId": "910ccd91-66fd-4874-bf34-9b6e66cfbc15"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Shape of X_train_scaled: {X_train_scaled.shape}\")\n",
        "print(f\"Shape of X_test_scaled: {X_test_scaled.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train_scaled: (4209, 359)\n",
            "Shape of X_test_scaled: (4209, 359)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28d187f9"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the data has been scaled, I will initialize a PCA model and fit it to the scaled training data (`X_train_scaled`). I will set `n_components=0.95` to retain 95% of the variance, and then transform both the scaled training and test datasets using this fitted PCA model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ca49447",
        "outputId": "16cc3cda-e00a-4a25-e34f-2878b9784776"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=0.95)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "print(f\"Original number of features: {X_train_scaled.shape[1]}\")\n",
        "print(f\"Number of components after PCA (train): {X_train_pca.shape[1]}\")\n",
        "print(f\"Number of components after PCA (test): {X_test_pca.shape[1]}\")\n",
        "print(f\"Explained variance ratio: {pca.explained_variance_ratio_.sum():.2f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original number of features: 359\n",
            "Number of components after PCA (train): 146\n",
            "Number of components after PCA (test): 146\n",
            "Explained variance ratio: 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71b614a3"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the PCA transformation has been applied, I will convert the PCA-transformed arrays into pandas DataFrames, `X_train_pca_df` and `X_test_pca_df`, and ensure they retain an appropriate index. This will make them ready for subsequent machine learning model training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c94e396",
        "outputId": "7d0f146e-934d-44f1-ecfd-c72058a53f9d"
      },
      "source": [
        "X_train_pca_df = pd.DataFrame(X_train_pca, index=X_train.index)\n",
        "X_test_pca_df = pd.DataFrame(X_test_pca, index=X_test.index)\n",
        "\n",
        "print(\"PCA-transformed data converted to DataFrames.\")\n",
        "print(f\"Shape of X_train_pca_df: {X_train_pca_df.shape}\")\n",
        "print(f\"Shape of X_test_pca_df: {X_test_pca_df.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PCA-transformed data converted to DataFrames.\n",
            "Shape of X_train_pca_df: (4209, 146)\n",
            "Shape of X_test_pca_df: (4209, 146)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf2f14a3"
      },
      "source": [
        "## Train XGBoost Model\n",
        "\n",
        "### Subtask:\n",
        "Train an XGBoost regression model using the preprocessed training data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4a10df5"
      },
      "source": [
        "**Reasoning**:\n",
        "I will import the XGBoost regressor, instantiate it with some initial parameters, and then fit the model using the PCA-transformed training features and the target variable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "982be279",
        "outputId": "67efc6ac-9482-4863-c097-1d612dd6dff2"
      },
      "source": [
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Instantiate XGBRegressor with some common parameters\n",
        "xgb_model = XGBRegressor(\n",
        "    n_estimators=1000, # Number of boosting rounds\n",
        "    learning_rate=0.05, # Step size shrinkage to prevent overfitting\n",
        "    max_depth=5, # Maximum depth of a tree\n",
        "    subsample=0.8, # Subsample ratio of the training instance\n",
        "    colsample_bytree=0.8, # Subsample ratio of columns when constructing each tree\n",
        "    random_state=42, # Random seed for reproducibility\n",
        "    n_jobs=-1 # Use all available CPU cores\n",
        ")\n",
        "\n",
        "# Fit the model to the PCA-transformed training data\n",
        "xgb_model.fit(X_train_pca_df, y_train)\n",
        "\n",
        "print(\"XGBoost Regressor model trained successfully.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Regressor model trained successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7fa9955"
      },
      "source": [
        "## Predict Test Set Values\n",
        "\n",
        "### Subtask:\n",
        "Use the trained XGBoost model to predict the test bench times for the preprocessed test dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49c28631"
      },
      "source": [
        "**Reasoning**:\n",
        "I will use the trained XGBoost model to make predictions on the PCA-transformed test dataset (`X_test_pca_df`) and store the results in `y_pred_test`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "802e3edf",
        "outputId": "892e628b-ebf3-4cbf-b4a8-abea91532ec7"
      },
      "source": [
        "y_pred_test = xgb_model.predict(X_test_pca_df)\n",
        "print(\"Predictions on test data generated successfully.\")\n",
        "print(f\"Shape of y_pred_test: {y_pred_test.shape}\")\n",
        "print(f\"First 5 predictions: {y_pred_test[:5]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions on test data generated successfully.\n",
            "Shape of y_pred_test: (4209,)\n",
            "First 5 predictions: [103.721275 118.361115 101.11171   80.81917  106.33356 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd8825bb"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the entire process, including the steps taken for data preparation, model training, and the final predictions made for the test set. Discuss potential next steps or insights from the model's performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e13dc6d1"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Data Loading and Initial Exploration**:\n",
        "    *   Successfully loaded `train.csv` (4209 rows, 378 columns) and `test.csv` (4209 rows, 377 columns) after unzipping.\n",
        "    *   Both datasets were found to have no missing values.\n",
        "    *   Identified 8 categorical columns common to both datasets ('X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X8').\n",
        "*   **Data Cleaning (Zero-Variance Columns)**:\n",
        "    *   17 unique columns with zero variance were identified across both training and test sets.\n",
        "    *   These 17 columns were removed, reducing `train_df` to (4209, 361) columns and `test_df` to (4209, 360) columns.\n",
        "*   **Feature Engineering (Label Encoding)**:\n",
        "    *   Label Encoding was applied to the 8 common categorical columns in both `train_df` and `test_df`, converting them to numerical representations.\n",
        "*   **Dimensionality Reduction (PCA)**:\n",
        "    *   Features were standardized using `StandardScaler`.\n",
        "    *   Principal Component Analysis (PCA) was applied to the standardized data, reducing the original 359 numerical features to 146 components while retaining 95% of the variance.\n",
        "    *   The PCA-transformed training and test feature sets (`X_train_pca_df`, `X_test_pca_df`) both have a shape of (4209, 146).\n",
        "*   **Model Training**:\n",
        "    *   An XGBoost Regressor model was trained using the PCA-transformed training features (`X_train_pca_df`) and the target variable (`y_train`).\n",
        "    *   Key hyperparameters included `n_estimators=1000`, `learning_rate=0.05`, and `max_depth=5`.\n",
        "*   **Prediction**:\n",
        "    *   The trained XGBoost model successfully generated predictions (`y_pred_test`) for the preprocessed test set (`X_test_pca_df`).\n",
        "    *   The predictions consist of 4209 values, with the first five predictions being approximately \\[103.72, 118.36, 101.11, 80.82, 106.33].\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The combination of Label Encoding for categorical features and PCA for numerical features effectively prepared the high-dimensional dataset for modeling. PCA significantly reduced the feature space from 359 to 146, which can help mitigate the curse of dimensionality and potentially improve model training efficiency without losing significant information.\n",
        "*   The next crucial steps involve evaluating the model's performance on a validation set (or using cross-validation) to assess its generalization capability and fine-tuning the XGBoost hyperparameters to optimize prediction accuracy.\n"
      ]
    }
  ]
}