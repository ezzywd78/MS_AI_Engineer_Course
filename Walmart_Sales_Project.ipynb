{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN5ulsUKQEscVkGOWTx4YgK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ezzywd78/MS_AI_Engineer_Course/blob/main/Walmart_Sales_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zCrMO0jXrgfd",
        "outputId": "ca5e3cda-b292-4d79-ed87-46cd5a331820"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Store with maximum total sales: 20\n",
            "Total sales for that store: $301,397,792.46\n",
            "\n",
            "Store with maximum standard deviation: 14\n",
            "Standard deviation: $317,569.95\n",
            "Store with maximum coefficient of variation (std/mean):\n",
            "Coefficient of variation: 0.2297\n",
            "\n",
            "Top 10 stores by QoQ growth (Q3 2012 vs Q2 2012):\n",
            "           Q3_2012      Q2_2012  QoQ_growth_Q3_2012_vs_Q2_2012\n",
            "Store                                                         \n",
            "7       8262787.39   7290859.27                       0.133308\n",
            "16      7121541.64   6564335.98                       0.084884\n",
            "35     11322421.12  10838313.00                       0.044666\n",
            "26     13675691.91  13155335.57                       0.039555\n",
            "39     20715116.23  20214128.46                       0.024784\n",
            "41     18093844.01  17659942.73                       0.024570\n",
            "44      4411251.16   4306405.78                       0.024346\n",
            "24     17976377.72  17684218.91                       0.016521\n",
            "40     12873195.37  12727737.53                       0.011428\n",
            "23     18641489.15  18488882.82                       0.008254\n",
            "\n",
            "Stores with positive QoQ growth into Q3 2012 (vs Q2 2012): [7, 16, 35, 26, 39, 41, 44, 24, 40, 23]\n",
            "\n",
            "Mean weekly sales across all stores during non-holiday weeks: $1,041,256.38\n",
            "\n",
            "Holiday weeks: total sales across stores and whether they exceed non-holiday average (all stores combined):\n",
            "        Holiday       Date  Total_Sales  HigherThanNonHolidayMean\n",
            "0    Super Bowl 2010-02-12  48336677.63                      True\n",
            "1    Super Bowl 2011-02-11  47336192.79                      True\n",
            "2    Super Bowl 2012-02-10  50009407.92                      True\n",
            "3    Labour Day 2010-09-10  45634397.84                     False\n",
            "4    Labour Day 2011-09-09  46763227.53                     False\n",
            "5    Labour Day 2012-09-07  48330059.31                      True\n",
            "6  Thanksgiving 2010-11-26  65821003.24                      True\n",
            "7  Thanksgiving 2011-11-25  66593605.26                      True\n",
            "8     Christmas 2010-12-31  40432519.00                     False\n",
            "9     Christmas 2011-12-30  46042461.04                     False\n",
            "\n",
            "Monthly totals (first 12 rows):\n",
            "             Total_Sales\n",
            "Date                    \n",
            "2010-02-01  1.903330e+08\n",
            "2010-03-01  1.819198e+08\n",
            "2010-04-01  2.314124e+08\n",
            "2010-05-01  1.867109e+08\n",
            "2010-06-01  1.922462e+08\n",
            "2010-07-01  2.325801e+08\n",
            "2010-08-01  1.876401e+08\n",
            "2010-09-01  1.772679e+08\n",
            "2010-10-01  2.171618e+08\n",
            "2010-11-01  2.028534e+08\n",
            "2010-12-01  2.887605e+08\n",
            "2011-01-01  1.637040e+08\n",
            "\n",
            "Semester totals by year (H1 / H2):\n",
            "Semester            H1            H2\n",
            "Year                                \n",
            "2010      9.826223e+08  1.306264e+09\n",
            "2011      1.127340e+09  1.320860e+09\n",
            "2012      1.210765e+09  7.893674e+08\n",
            "\n",
            "Saved plots: monthly_total_sales.png and semester_sales_by_year.png\n",
            "\n",
            "Store 1 prediction results (last 12 weeks as test):\n",
            "Linear Regression  RMSE: $65,268.78, MAE: $56,145.14\n",
            "Random Forest     RMSE: $67,613.38, MAE: $53,816.88\n",
            "Selected best model based on RMSE: LinearRegression\n",
            "\n",
            "Linear Regression coefficients (by absolute magnitude):\n",
            "        feature          coef\n",
            "2  Unemployment  95404.285182\n",
            "5  Holiday_Flag  90272.799010\n",
            "3    Fuel_Price -76616.252940\n",
            "6     month_sin  42673.023019\n",
            "7     month_cos  18419.758903\n",
            "1           CPI   6720.290973\n",
            "4   Temperature   -271.763840\n",
            "0     day_index    237.130953\n",
            "\n",
            "Random Forest feature importances:\n",
            "        feature  importance\n",
            "7     month_cos    0.210298\n",
            "1           CPI    0.179270\n",
            "4   Temperature    0.173436\n",
            "0     day_index    0.150331\n",
            "3    Fuel_Price    0.140649\n",
            "5  Holiday_Flag    0.100543\n",
            "6     month_sin    0.025214\n",
            "2  Unemployment    0.020259\n",
            "\n",
            "Saved predictions to store1_predictions_last_12weeks.csv\n",
            "Saved plot store1_predictions_comparison.png\n",
            "\n",
            "Wrote automated insights to insights_summary.txt\n",
            "\n",
            "Wrote recommended next steps to next_steps.txt\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x400 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Retail Analysis with Walmart Data\n",
        "# Course-end Project 1 — reproducible script\n",
        "#\n",
        "# Save this file next to Walmart_Store_sales.csv and run:\n",
        "#   pip install pandas numpy matplotlib seaborn scikit-learn joblib\n",
        "#   python retail_analysis_walmart.py\n",
        "#\n",
        "# The script:\n",
        "# - computes the requested basic statistics\n",
        "# - produces monthly / semester aggregates\n",
        "# - maps the holiday weeks to holiday names and compares their sales vs non-holiday mean\n",
        "# - fits simple prediction models for Store 1 and reports RMSE/MAE, selecting the best model\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# ---- 1. Load and prepare data ----\n",
        "df = pd.read_csv(\"Walmart_Store_sales.csv\")\n",
        "df['Date'] = pd.to_datetime(df['Date'], format=\"%d-%m-%Y\")  # file dates are in d-m-Y\n",
        "df = df.sort_values(['Store', 'Date']).reset_index(drop=True)\n",
        "\n",
        "# Additional time columns\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Quarter'] = df['Date'].dt.to_period('Q')\n",
        "df['WeekOfYear'] = df['Date'].dt.isocalendar().week.astype(int)\n",
        "\n",
        "# ---- 2. Basic statistics tasks ----\n",
        "\n",
        "# 2.1 Which store has maximum sales (total across period)?\n",
        "store_total = df.groupby('Store')['Weekly_Sales'].sum().sort_values(ascending=False)\n",
        "store_with_max_sales = store_total.idxmax()\n",
        "print(\"Store with maximum total sales:\", store_with_max_sales)\n",
        "print(\"Total sales for that store: ${:,.2f}\".format(store_total.loc[store_with_max_sales]))\n",
        "\n",
        "# 2.2 Which store has maximum standard deviation i.e., sales vary a lot.\n",
        "store_std = df.groupby('Store')['Weekly_Sales'].std().sort_values(ascending=False)\n",
        "store_with_max_std = store_std.idxmax()\n",
        "store_mean = df.groupby('Store')['Weekly_Sales'].mean()\n",
        "coef_var = (store_std / store_mean).sort_values(ascending=False)  # coefficient of variation = std/mean\n",
        "store_with_max_cv = coef_var.idxmax()\n",
        "print(\"\\nStore with maximum standard deviation:\", store_with_max_std)\n",
        "print(\"Standard deviation: ${:,.2f}\".format(store_std.loc[store_with_max_std]))\n",
        "print(\"Store with maximum coefficient of variation (std/mean):\".format(store_with_max_cv))\n",
        "print(\"Coefficient of variation: {:.4f}\".format(coef_var.loc[store_with_max_cv]))\n",
        "\n",
        "# 2.3 Which store(s) has good quarterly growth rate in Q3 2012\n",
        "# We'll calculate two kinds of growth:\n",
        "#  - QoQ: (Q3_2012 - Q2_2012) / Q2_2012\n",
        "#  - YoY: (Q3_2012 - Q3_2011) / Q3_2011 (year-over-year)\n",
        "q_sales = df.copy()\n",
        "q_sales['YearQuarter'] = q_sales['Date'].dt.to_period('Q')\n",
        "q_agg = q_sales.groupby(['Store', 'YearQuarter'])['Weekly_Sales'].sum().unstack(fill_value=np.nan)\n",
        "\n",
        "q3_2012 = q_agg.get('2012Q3')  # Period label format \"YYYYQn\"\n",
        "q2_2012 = q_agg.get('2012Q2')\n",
        "q3_2011 = q_agg.get('2011Q3')\n",
        "\n",
        "# compute growth safely (avoid division by zero)\n",
        "qoQ = ((q3_2012 - q2_2012) / q2_2012).replace([np.inf, -np.inf], np.nan)\n",
        "yoY = ((q3_2012 - q3_2011) / q3_2011).replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# pick stores with positive growth (both metrics positive) and rank them:\n",
        "growth_df = pd.DataFrame({\n",
        "    'Q3_2012': q3_2012,\n",
        "    'Q2_2012': q2_2012,\n",
        "    'Q3_2011': q3_2011,\n",
        "    'QoQ_growth_Q3_2012_vs_Q2_2012': qoQ,\n",
        "    'YoY_growth_Q3_2012_vs_Q3_2011': yoY\n",
        "}).sort_values('QoQ_growth_Q3_2012_vs_Q2_2012', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 stores by QoQ growth (Q3 2012 vs Q2 2012):\")\n",
        "print(growth_df[['Q3_2012','Q2_2012','QoQ_growth_Q3_2012_vs_Q2_2012']].head(10))\n",
        "\n",
        "# stores with clearly positive QoQ growth:\n",
        "stores_positive_qoq = growth_df[growth_df['QoQ_growth_Q3_2012_vs_Q2_2012'] > 0].index.tolist()\n",
        "print(\"\\nStores with positive QoQ growth into Q3 2012 (vs Q2 2012):\", stores_positive_qoq)\n",
        "\n",
        "# 2.4 Holidays with higher sales than mean non-holiday sales\n",
        "# Define holiday dates mapping (from problem statement) and filter only dates present in data\n",
        "holiday_map = {\n",
        "    'Super Bowl': ['2010-02-12', '2011-02-11', '2012-02-10'],\n",
        "    'Labour Day': ['2010-09-10', '2011-09-09', '2012-09-07'],\n",
        "    'Thanksgiving': ['2010-11-26', '2011-11-25', '2012-11-23'],\n",
        "    'Christmas': ['2010-12-31', '2011-12-30']  # 2012-12-28 is outside dataset end maybe\n",
        "}\n",
        "# normalize map to datetimes that exist in the dataset\n",
        "holiday_rows = []\n",
        "for hname, dates in holiday_map.items():\n",
        "    for d in dates:\n",
        "        try:\n",
        "            dt = pd.to_datetime(d, format=\"%Y-%m-%d\")\n",
        "            if dt in set(df['Date']):\n",
        "                holiday_rows.append((hname, dt))\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "# mean sales for non-holiday weeks (all stores together)\n",
        "non_holiday_mean = df[df['Holiday_Flag'] == 0]['Weekly_Sales'].mean()\n",
        "print(\"\\nMean weekly sales across all stores during non-holiday weeks: ${:,.2f}\".format(non_holiday_mean))\n",
        "\n",
        "# compute avg total sales across all stores for each holiday date\n",
        "holiday_stats = []\n",
        "for hname, dt in holiday_rows:\n",
        "    # sum across all stores for that week\n",
        "    total_sales = df[df['Date'] == dt]['Weekly_Sales'].sum()\n",
        "    holiday_stats.append({'Holiday': hname, 'Date': dt, 'Total_Sales': total_sales})\n",
        "\n",
        "holiday_stats_df = pd.DataFrame(holiday_stats)\n",
        "holiday_stats_df['HigherThanNonHolidayMean'] = holiday_stats_df['Total_Sales'] > non_holiday_mean * df['Store'].nunique()\n",
        "# Note: non_holiday_mean is per-store-week; to compare to total across all stores multiply by number of stores.\n",
        "print(\"\\nHoliday weeks: total sales across stores and whether they exceed non-holiday average (all stores combined):\")\n",
        "print(holiday_stats_df[['Holiday','Date','Total_Sales','HigherThanNonHolidayMean']])\n",
        "\n",
        "# 2.5 Monthly and semester view of sales in units and insights\n",
        "# Monthly totals (all stores combined)\n",
        "monthly = df.groupby([df['Date'].dt.to_period('M')])['Weekly_Sales'].sum().rename('Total_Sales').to_frame()\n",
        "monthly.index = monthly.index.to_timestamp()\n",
        "print(\"\\nMonthly totals (first 12 rows):\")\n",
        "print(monthly.head(12))\n",
        "\n",
        "# Semester view: define H1 = Jan-Jun, H2 = Jul-Dec per year\n",
        "df['Semester'] = np.where(df['Month'] <= 6, 'H1', 'H2')\n",
        "semester_totals = df.groupby(['Year','Semester'])['Weekly_Sales'].sum().unstack().fillna(0)\n",
        "print(\"\\nSemester totals by year (H1 / H2):\")\n",
        "print(semester_totals)\n",
        "\n",
        "# Plotting monthly and semester (save figures)\n",
        "plt.figure(figsize=(12,5))\n",
        "monthly.plot(kind='line', legend=False, title='Total monthly sales (all stores)')\n",
        "plt.ylabel('Total Weekly Sales (sum across stores)')\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"monthly_total_sales.png\", dpi=150)\n",
        "plt.close()\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "semester_totals.plot(kind='bar', title='Semester sales by Year (H1 vs H2)')\n",
        "plt.ylabel('Total Sales')\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"semester_sales_by_year.png\", dpi=150)\n",
        "plt.close()\n",
        "\n",
        "print(\"\\nSaved plots: monthly_total_sales.png and semester_sales_by_year.png\")\n",
        "\n",
        "# ---- 3. Statistical Model for Store 1 ----\n",
        "# We'll prepare data, create a day-index, basic features, and compare Linear Regression vs RandomForest.\n",
        "store1 = df[df['Store'] == 1].copy().sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "# create a numeric date index: days since first date\n",
        "start_date = store1['Date'].min()\n",
        "store1['day_index'] = (store1['Date'] - start_date).dt.days\n",
        "\n",
        "# features: day_index, CPI, Unemployment, Fuel_Price, Temperature, Holiday_Flag, Month (cyclical)\n",
        "store1['month_sin'] = np.sin(2 * np.pi * store1['Month'] / 12)\n",
        "store1['month_cos'] = np.cos(2 * np.pi * store1['Month'] / 12)\n",
        "\n",
        "features = ['day_index', 'CPI', 'Unemployment', 'Fuel_Price', 'Temperature', 'Holiday_Flag', 'month_sin', 'month_cos']\n",
        "X = store1[features].ffill().bfill()  # Use ffill() and bfill() directly\n",
        "y = store1['Weekly_Sales']\n",
        "\n",
        "# Time-aware train/test split: last 12 weeks as test\n",
        "test_weeks = 12\n",
        "X_train, X_test = X[:-test_weeks], X[-test_weeks:]\n",
        "y_train, y_test = y[:-test_weeks], y[-test_weeks:]\n",
        "\n",
        "# Model 1: Linear Regression\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
        "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
        "\n",
        "# Model 2: Random Forest\n",
        "rf = RandomForestRegressor(n_estimators=200, random_state=RANDOM_STATE)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
        "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "\n",
        "print(\"\\nStore 1 prediction results (last {} weeks as test):\".format(test_weeks))\n",
        "print(\"Linear Regression  RMSE: ${:,.2f}, MAE: ${:,.2f}\".format(rmse_lr, mae_lr))\n",
        "print(\"Random Forest     RMSE: ${:,.2f}, MAE: ${:,.2f}\".format(rmse_rf, mae_rf))\n",
        "\n",
        "# Choose best model\n",
        "best_model_name = \"LinearRegression\" if rmse_lr < rmse_rf else \"RandomForest\"\n",
        "print(\"Selected best model based on RMSE:\", best_model_name)\n",
        "\n",
        "# show linear regression coefficients (if chosen)\n",
        "coef_df = pd.DataFrame({'feature': features, 'coef': lr.coef_}).sort_values('coef', key=lambda s: s.abs(), ascending=False)\n",
        "print(\"\\nLinear Regression coefficients (by absolute magnitude):\")\n",
        "print(coef_df)\n",
        "\n",
        "# Feature importances from RF\n",
        "fi = pd.DataFrame({'feature': features, 'importance': rf.feature_importances_}).sort_values('importance', ascending=False)\n",
        "print(\"\\nRandom Forest feature importances:\")\n",
        "print(fi)\n",
        "\n",
        "# Save predictions to CSV for inspection\n",
        "predictions = store1[['Date', 'Weekly_Sales']].copy().iloc[-test_weeks:].reset_index(drop=True)\n",
        "predictions['LR_Pred'] = y_pred_lr\n",
        "predictions['RF_Pred'] = y_pred_rf\n",
        "predictions.to_csv(\"store1_predictions_last_{}weeks.csv\".format(test_weeks), index=False)\n",
        "print(\"\\nSaved predictions to store1_predictions_last_{}weeks.csv\".format(test_weeks))\n",
        "\n",
        "# Plot predictions vs actual for store 1 (last test_weeks)\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(predictions['Date'], predictions['Weekly_Sales'], label='Actual', marker='o')\n",
        "plt.plot(predictions['Date'], predictions['LR_Pred'], label='LinearRegression', marker='x')\n",
        "plt.plot(predictions['Date'], predictions['RF_Pred'], label='RandomForest', marker='s')\n",
        "plt.legend()\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Weekly Sales')\n",
        "plt.title('Store 1: actual vs predicted (last {} weeks)'.format(test_weeks))\n",
        "plt.xticks(rotation=30)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"store1_predictions_comparison.png\", dpi=150)\n",
        "plt.close()\n",
        "print(\"Saved plot store1_predictions_comparison.png\")\n",
        "\n",
        "# ---- 4. Quick notes / insights (automated suggestions) ----\n",
        "insights = \"\"\"\n",
        "Automated insights summary:\n",
        "- Store with maximum total sales: {} (see store_total series).\n",
        "- Store with maximum variability (std): {}.\n",
        "- Several stores showed positive QoQ growth into Q3 2012 (list printed).\n",
        "- Holiday weeks comparison: see holiday_stats_df printed above; those holiday weeks with totals above\n",
        "  the aggregate non-holiday mean (scaled to number of stores) are flagged.\n",
        "- Monthly and semester aggregates saved as PNGs. Inspect monthly_total_sales.png for seasonality,\n",
        "  and semester_sales_by_year.png for H1/H2 comparisons.\n",
        "- Modeling (Store 1): compared a temporal linear model vs RandomForest (features: day index, CPI,\n",
        "  Unemployment, Fuel_Price, Temperature, Holiday_Flag, cyclical month). The script reports RMSE/MAE.\n",
        "  RandomForest often captures non-linearity and interactions; LinearRegression is interpretable (coeffs printed).\n",
        "\"\"\".format(store_with_max_sales, store_with_max_std)\n",
        "\n",
        "with open(\"insights_summary.txt\",\"w\") as f:\n",
        "    f.write(insights)\n",
        "\n",
        "print(\"\\nWrote automated insights to insights_summary.txt\")\n",
        "\n",
        "# ---- 5. Recommendations & next steps ----\n",
        "recommendations = \"\"\"\n",
        "Recommended next steps (for better accuracy and business value):\n",
        "1) Add lag features and rolling means (e.g., sales lag-1, lag-2, 4-week rolling mean) to capture temporal autocorrelation.\n",
        "2) Use time-series-specific models: SARIMAX (with exogenous variables), FB Prophet (now called 'prophet'),\n",
        "   or gradient boosting (XGBoost/LightGBM) with time features + lags.\n",
        "3) Engineer markdown/promotion features: the dataset lacks explicit markdown amounts — try to infer markdown events\n",
        "   from sudden sales uplift patterns or introduce retailer markup calendars if available.\n",
        "4) Cross-validate using rolling (time-series) CV instead of a single holdout; tune hyperparameters for tree-based models.\n",
        "5) Evaluate business metric: weight holiday-week errors more (per problem statement) — e.g., multiply errors on\n",
        "   the four major holiday weeks by 5 to reflect competition scoring.\n",
        "6) For supply chain decisions, produce prediction intervals (quantiles) instead of point estimates.\n",
        "\"\"\"\n",
        "\n",
        "with open(\"next_steps.txt\",\"w\") as f:\n",
        "    f.write(recommendations)\n",
        "\n",
        "print(\"\\nWrote recommended next steps to next_steps.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "67fe7879",
        "outputId": "b5471ef6-f714-453b-d173-0aa1bf5343cd"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('retail_analysis_report.pdf')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6124c5d1-4860-42ef-b0da-aa8811786a69\", \"retail_analysis_report.pdf\", 223695)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "0ec252f3",
        "outputId": "d88ea93d-29af-4e24-b79f-46c87451dc07"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('retail_analysis_report.pdf')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7484ff5c-6e50-464a-aa79-98e7efd4c133\", \"retail_analysis_report.pdf\", 214648)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e791fc57",
        "outputId": "56898c85-00e1-4d47-edd6-076120f95188"
      },
      "source": [
        "pip install fpdf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fpdf\n",
            "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: fpdf\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=e1e4d652c1e4e7caab6a495799dd30fd028fdad2414eab082dd5ca8ca49aa58c\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/62/11/dc73d78e40a218ad52e7451f30166e94491be013a7850b5d75\n",
            "Successfully built fpdf\n",
            "Installing collected packages: fpdf\n",
            "Successfully installed fpdf-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73cc18c7",
        "outputId": "4700eafa-e47c-4a53-b96d-eea7d076fc59"
      },
      "source": [
        "from fpdf import FPDF\n",
        "\n",
        "def create_report_pdf(output_filename=\"retail_analysis_report.pdf\"):\n",
        "    pdf = FPDF()\n",
        "    pdf.set_auto_page_break(auto=True, margin=15)\n",
        "    pdf.add_page()\n",
        "\n",
        "    # Title\n",
        "    pdf.set_font(\"Arial\", \"B\", 16)\n",
        "    pdf.cell(0, 10, \"Walmart Store Sales Retail Analysis Report\", 0, 1, \"C\")\n",
        "    pdf.ln(10)\n",
        "\n",
        "    # Add Insights Summary\n",
        "    pdf.set_font(\"Arial\", \"B\", 12)\n",
        "    pdf.cell(0, 10, \"Insights Summary\", 0, 1, \"L\") # Renamed from \"Automated Insights Summary\"\n",
        "    pdf.set_font(\"Arial\", \"\", 10)\n",
        "    try:\n",
        "        with open(\"insights_summary.txt\", \"r\") as f:\n",
        "            insights_text = f.read()\n",
        "        # Replace problematic character for latin-1 encoding\n",
        "        insights_text = insights_text.replace('—', '--')\n",
        "        pdf.multi_cell(0, 5, insights_text)\n",
        "    except FileNotFoundError:\n",
        "        pdf.multi_cell(0, 5, \"Insights summary file not found.\")\n",
        "    pdf.ln(5)\n",
        "\n",
        "    # Removed 'Add Next Steps' section\n",
        "    # pdf.set_font(\"Arial\", \"B\", 12)\n",
        "    # pdf.cell(0, 10, \"Recommended Next Steps\", 0, 1, \"L\")\n",
        "    # pdf.set_font(\"Arial\", \"\", 10)\n",
        "    # try:\n",
        "    #     with open(\"next_steps.txt\", \"r\") as f:\n",
        "    #         next_steps_text = f.read()\n",
        "    #     # Replace problematic character for latin-1 encoding\n",
        "    #     next_steps_text = next_steps_text.replace('—', '--')\n",
        "    #     pdf.multi_cell(0, 5, next_steps_text)\n",
        "    # except FileNotFoundError:\n",
        "    #     pdf.multi_cell(0, 5, \"Next steps file not found.\")\n",
        "    # pdf.ln(5)\n",
        "\n",
        "    # Add Monthly Total Sales Plot\n",
        "    pdf.add_page()\n",
        "    pdf.set_font(\"Arial\", \"B\", 12)\n",
        "    pdf.cell(0, 10, \"Monthly Total Sales\", 0, 1, \"C\")\n",
        "    try:\n",
        "        pdf.image(\"monthly_total_sales.png\", x=10, y=pdf.get_y(), w=190)\n",
        "        pdf.set_y(pdf.get_y() + 100) # Adjust y position after image\n",
        "    except FileNotFoundError:\n",
        "        pdf.multi_cell(0, 5, \"Monthly sales plot not found.\")\n",
        "    pdf.ln(10)\n",
        "\n",
        "    # Add Semester Sales by Year Plot\n",
        "    pdf.add_page()\n",
        "    pdf.set_font(\"Arial\", \"B\", 12)\n",
        "    pdf.cell(0, 10, \"Semester Sales by Year\", 0, 1, \"C\")\n",
        "    try:\n",
        "        pdf.image(\"semester_sales_by_year.png\", x=10, y=pdf.get_y(), w=190)\n",
        "        pdf.set_y(pdf.get_y() + 100) # Adjust y position after image\n",
        "    except FileNotFoundError:\n",
        "        pdf.multi_cell(0, 5, \"Semester sales plot not found.\")\n",
        "    pdf.ln(10)\n",
        "\n",
        "    # Add Store 1 Predictions Comparison Plot\n",
        "    pdf.add_page()\n",
        "    pdf.set_font(\"Arial\", \"B\", 12)\n",
        "    pdf.cell(0, 10, \"Store 1: Actual vs Predicted Sales\", 0, 1, \"C\")\n",
        "    try:\n",
        "        pdf.image(\"store1_predictions_comparison.png\", x=10, y=pdf.get_y(), w=190)\n",
        "        pdf.set_y(pdf.get_y() + 100) # Adjust y position after image\n",
        "    except FileNotFoundError:\n",
        "        pdf.multi_cell(0, 5, \"Store 1 predictions plot not found.\")\n",
        "    pdf.ln(10)\n",
        "\n",
        "    # Add Python Code\n",
        "    pdf.add_page()\n",
        "    pdf.set_font(\"Arial\", \"B\", 12)\n",
        "    pdf.cell(0, 10, \"Python Analysis Script\", 0, 1, \"C\")\n",
        "    pdf.ln(5)\n",
        "    pdf.set_font(\"Courier\", \"\", 8) # Use a monospace font for code\n",
        "\n",
        "    # Get the content of the main analysis script cell directly\n",
        "    # The cell ID 'zCrMO0jXrgfd' is assumed to contain the analysis script\n",
        "    # This avoids issues with embedding multi-line strings within multi-line strings\n",
        "    # For this to work, the agent must have access to the notebook_state to retrieve cell content.\n",
        "    # In a real execution environment, you might read from a file or a global variable.\n",
        "    # For the purposes of this interaction, I will manually provide the content from the previous steps.\n",
        "    code_content = '''\n",
        "# Retail Analysis with Walmart Data\n",
        "# Course-end Project 1 — reproducible script\n",
        "#\n",
        "# Save this file next to Walmart_Store_sales.csv and run:\n",
        "#   pip install pandas numpy matplotlib seaborn scikit-learn joblib\n",
        "#   python retail_analysis_walmart.py\n",
        "#\n",
        "# The script:\n",
        "# - computes the requested basic statistics\n",
        "# - produces monthly / semester aggregates\n",
        "# - maps the holiday weeks to holiday names and compares their sales vs non-holiday mean\n",
        "# - fits simple prediction models for Store 1 and reports RMSE/MAE, selecting the best model\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# ---- 1. Load and prepare data ----\n",
        "df = pd.read_csv(\"Walmart_Store_sales.csv\")\n",
        "df['Date'] = pd.to_datetime(df['Date'], format=\"%d-%m-%Y\")  # file dates are in d-m-Y\n",
        "df = df.sort_values(['Store', 'Date']).reset_index(drop=True)\n",
        "\n",
        "# Additional time columns\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Quarter'] = df['Date'].dt.to_period('Q')\n",
        "df['WeekOfYear'] = df['Date'].dt.isocalendar().week.astype(int)\n",
        "\n",
        "# ---- 2. Basic statistics tasks ----\n",
        "\n",
        "# 2.1 Which store has maximum sales (total across period)?\n",
        "store_total = df.groupby('Store')['Weekly_Sales'].sum().sort_values(ascending=False)\n",
        "store_with_max_sales = store_total.idxmax()\n",
        "print(\"Store with maximum total sales:\", store_with_max_sales)\n",
        "print(\"Total sales for that store: ${:,.2f}\".format(store_total.loc[store_with_max_sales]))\n",
        "\n",
        "# 2.2 Which store has maximum standard deviation i.e., sales vary a lot.\n",
        "store_std = df.groupby('Store')['Weekly_Sales'].std().sort_values(ascending=False)\n",
        "store_with_max_std = store_std.idxmax()\n",
        "store_mean = df.groupby('Store')['Weekly_Sales'].mean()\n",
        "coef_var = (store_std / store_mean).sort_values(ascending=False)  # coefficient of variation = std/mean\n",
        "store_with_max_cv = coef_var.idxmax()\n",
        "print(\"\\nStore with maximum standard deviation:\", store_with_max_std)\n",
        "print(\"Standard deviation: ${:,.2f}\".format(store_std.loc[store_with_max_std]))\n",
        "print(\"Store with maximum coefficient of variation (std/mean):\".format(store_with_max_cv))\n",
        "print(\"Coefficient of variation: {:.4f}\".format(coef_var.loc[store_with_max_cv]))\n",
        "\n",
        "# 2.3 Which store(s) has good quarterly growth rate in Q3 2012\n",
        "# We'll calculate two kinds of growth:\n",
        "#  - QoQ: (Q3_2012 - Q2_2012) / Q2_2012\n",
        "#  - YoY: (Q3_2012 - Q3_2011) / Q3_2011 (year-over-year)\n",
        "q_sales = df.copy()\n",
        "q_sales['YearQuarter'] = q_sales['Date'].dt.to_period('Q')\n",
        "q_agg = q_sales.groupby(['Store', 'YearQuarter'])['Weekly_Sales'].sum().unstack(fill_value=np.nan)\n",
        "\n",
        "q3_2012 = q_agg.get('2012Q3')  # Period label format \"YYYYQn\"\n",
        "q2_2012 = q_agg.get('2012Q2')\n",
        "q3_2011 = q_agg.get('2011Q3')\n",
        "\n",
        "# compute growth safely (avoid division by zero)\n",
        "qoQ = ((q3_2012 - q2_2012) / q2_2012).replace([np.inf, -np.inf], np.nan)\n",
        "yoY = ((q3_2012 - q3_2011) / q3_2011).replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# pick stores with positive growth (both metrics positive) and rank them:\n",
        "growth_df = pd.DataFrame({\n",
        "    'Q3_2012': q3_2012,\n",
        "    'Q2_2012': q2_2012,\n",
        "    'Q3_2011': q3_2011,\n",
        "    'QoQ_growth_Q3_2012_vs_Q2_2012': qoQ,\n",
        "    'YoY_growth_Q3_2012_vs_Q3_2011': yoY\n",
        "}).sort_values('QoQ_growth_Q3_2012_vs_Q2_2012', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 stores by QoQ growth (Q3 2012 vs Q2 2012):\")\n",
        "print(growth_df[['Q3_2012','Q2_2012','QoQ_growth_Q3_2012_vs_Q2_2012']].head(10))\n",
        "\n",
        "# stores with clearly positive QoQ growth:\n",
        "stores_positive_qoq = growth_df[growth_df['QoQ_growth_Q3_2012_vs_Q2_2012'] > 0].index.tolist()\n",
        "print(\"\\nStores with positive QoQ growth into Q3 2012 (vs Q2 2012):\", stores_positive_qoq)\n",
        "\n",
        "# 2.4 Holidays with higher sales than mean non-holiday sales\n",
        "# Define holiday dates mapping (from problem statement) and filter only dates present in data\n",
        "holiday_map = {\n",
        "    'Super Bowl': ['2010-02-12', '2011-02-11', '2012-02-10'],\n",
        "    'Labour Day': ['2010-09-10', '2011-09-09', '2012-09-07'],\n",
        "    'Thanksgiving': ['2010-11-26', '2011-11-25', '2012-11-23'],\n",
        "    'Christmas': ['2010-12-31', '2011-12-30']  # 2012-12-28 is outside dataset end maybe\n",
        "}\n",
        "# normalize map to datetimes that exist in the dataset\n",
        "holiday_rows = []\n",
        "for hname, dates in holiday_map.items():\n",
        "    for d in dates:\n",
        "        try:\n",
        "            dt = pd.to_datetime(d, format=\"%Y-%m-%d\")\n",
        "            if dt in set(df['Date']):\n",
        "                holiday_rows.append((hname, dt))\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "# mean sales for non-holiday weeks (all stores together)\n",
        "non_holiday_mean = df[df['Holiday_Flag'] == 0]['Weekly_Sales'].mean()\n",
        "print(\"\\nMean weekly sales across all stores during non-holiday weeks: ${:,.2f}\".format(non_holiday_mean))\n",
        "\n",
        "# compute avg total sales across all stores for each holiday date\n",
        "holiday_stats = []\n",
        "for hname, dt in holiday_rows:\n",
        "    # sum across all stores for that week\n",
        "    total_sales = df[df['Date'] == dt]['Weekly_Sales'].sum()\n",
        "    holiday_stats.append({'Holiday': hname, 'Date': dt, 'Total_Sales': total_sales})\n",
        "\n",
        "holiday_stats_df = pd.DataFrame(holiday_stats)\n",
        "holiday_stats_df['HigherThanNonHolidayMean'] = holiday_stats_df['Total_Sales'] > non_holiday_mean * df['Store'].nunique()\n",
        "# Note: non_holiday_mean is per-store-week; to compare to total across all stores multiply by number of stores.\n",
        "print(\"\\nHoliday weeks: total sales across stores and whether they exceed non-holiday average (all stores combined):\")\n",
        "print(holiday_stats_df[['Holiday','Date','Total_Sales','HigherThanNonHolidayMean']])\n",
        "\n",
        "# 2.5 Monthly and semester view of sales in units and insights\n",
        "# Monthly totals (all stores combined)\n",
        "monthly = df.groupby([df['Date'].dt.to_period('M')])['Weekly_Sales'].sum().rename('Total_Sales').to_frame()\n",
        "monthly.index = monthly.index.to_timestamp()\n",
        "print(\"\\nMonthly totals (first 12 rows):\")\n",
        "print(monthly.head(12))\n",
        "\n",
        "# Semester view: define H1 = Jan-Jun, H2 = Jul-Dec per year\n",
        "df['Semester'] = np.where(df['Month'] <= 6, 'H1', 'H2')\n",
        "semester_totals = df.groupby(['Year','Semester'])['Weekly_Sales'].sum().unstack().fillna(0)\n",
        "print(\"\\nSemester totals by year (H1 / H2):\")\n",
        "print(semester_totals)\n",
        "\n",
        "# Plotting monthly and semester (save figures)\n",
        "plt.figure(figsize=(12,5))\n",
        "monthly.plot(kind='line', legend=False, title='Total monthly sales (all stores)')\n",
        "plt.ylabel('Total Weekly Sales (sum across stores)')\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"monthly_total_sales.png\", dpi=150)\n",
        "plt.close()\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "semester_totals.plot(kind='bar', title='Semester sales by Year (H1 vs H2)')\n",
        "plt.ylabel('Total Sales')\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"semester_sales_by_year.png\", dpi=150)\n",
        "plt.close()\n",
        "\n",
        "print(\"\\nSaved plots: monthly_total_sales.png and semester_sales_by_year.png\")\n",
        "\n",
        "# ---- 3. Statistical Model for Store 1 ----\n",
        "# We'll prepare data, create a day-index, basic features, and compare Linear Regression vs RandomForest.\n",
        "store1 = df[df['Store'] == 1].copy().sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "# create a numeric date index: days since first date\n",
        "start_date = store1['Date'].min()\n",
        "store1['day_index'] = (store1['Date'] - start_date).dt.days\n",
        "\n",
        "# features: day_index, CPI, Unemployment, Fuel_Price, Temperature, Holiday_Flag, Month (cyclical)\n",
        "store1['month_sin'] = np.sin(2 * np.pi * store1['Month'] / 12)\n",
        "store1['month_cos'] = np.cos(2 * np.pi * store1['Month'] / 12)\n",
        "\n",
        "features = ['day_index', 'CPI', 'Unemployment', 'Fuel_Price', 'Temperature', 'Holiday_Flag', 'month_sin', 'month_cos']\n",
        "X = store1[features].ffill().bfill()  # Use ffill() and bfill() directly\n",
        "y = store1['Weekly_Sales']\n",
        "\n",
        "# Time-aware train/test split: last 12 weeks as test\n",
        "test_weeks = 12\n",
        "X_train, X_test = X[:-test_weeks], X[-test_weeks:]\n",
        "y_train, y_test = y[:-test_weeks], y[-test_weeks:]\n",
        "\n",
        "# Model 1: Linear Regression\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
        "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
        "\n",
        "# Model 2: Random Forest\n",
        "rf = RandomForestRegressor(n_estimators=200, random_state=RANDOM_STATE)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
        "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "\n",
        "print(\"\\nStore 1 prediction results (last {} weeks as test):\".format(test_weeks))\n",
        "print(\"Linear Regression  RMSE: ${:,.2f}, MAE: ${:,.2f}\".format(rmse_lr, mae_lr))\n",
        "print(\"Random Forest     RMSE: ${:,.2f}, MAE: ${:,.2f}\".format(rmse_rf, mae_rf))\n",
        "\n",
        "# Choose best model\n",
        "best_model_name = \"LinearRegression\" if rmse_lr < rmse_rf else \"RandomForest\"\n",
        "print(\"Selected best model based on RMSE:\", best_model_name)\n",
        "\n",
        "# show linear regression coefficients (if chosen)\n",
        "coef_df = pd.DataFrame({'feature': features, 'coef': lr.coef_}).sort_values('coef', key=lambda s: s.abs(), ascending=False)\n",
        "print(\"\\nLinear Regression coefficients (by absolute magnitude):\")\n",
        "print(coef_df)\n",
        "\n",
        "# Feature importances from RF\n",
        "fi = pd.DataFrame({'feature': features, 'importance': rf.feature_importances_}).sort_values('importance', ascending=False)\n",
        "print(\"\\nRandom Forest feature importances:\")\n",
        "print(fi)\n",
        "\n",
        "# Save predictions to CSV for inspection\n",
        "predictions = store1[['Date', 'Weekly_Sales']].copy().iloc[-test_weeks:].reset_index(drop=True)\n",
        "predictions['LR_Pred'] = y_pred_lr\n",
        "predictions['RF_Pred'] = y_pred_rf\n",
        "predictions.to_csv(\"store1_predictions_last_{}weeks.csv\".format(test_weeks), index=False)\n",
        "print(\"\\nSaved predictions to store1_predictions_last_{}weeks.csv\".format(test_weeks))\n",
        "\n",
        "# Plot predictions vs actual for store 1 (last test_weeks)\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(predictions['Date'], predictions['Weekly_Sales'], label='Actual', marker='o')\n",
        "plt.plot(predictions['Date'], predictions['LR_Pred'], label='LinearRegression', marker='x')\n",
        "plt.plot(predictions['Date'], predictions['RF_Pred'], label='RandomForest', marker='s')\n",
        "plt.legend()\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Weekly Sales')\n",
        "plt.title('Store 1: actual vs predicted (last {} weeks)'.format(test_weeks))\n",
        "plt.xticks(rotation=30)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"store1_predictions_comparison.png\", dpi=150)\n",
        "plt.close()\n",
        "print(\"Saved plot store1_predictions_comparison.png\")\n",
        "\n",
        "# ---- 4. Quick notes / insights (automated suggestions) ----\n",
        "insights = \"\"\"\n",
        "Automated insights summary:\n",
        "- Store with maximum total sales: {} (see store_total series).\n",
        "- Store with maximum variability (std): {}.\n",
        "- Several stores showed positive QoQ growth into Q3 2012 (list printed).\n",
        "- Holiday weeks comparison: see holiday_stats_df printed above; those holiday weeks with totals above\n",
        "  the aggregate non-holiday mean (scaled to number of stores) are flagged.\n",
        "- Monthly and semester aggregates saved as PNGs. Inspect monthly_total_sales.png for seasonality,\n",
        "  and semester_sales_by_year.png for H1/H2 comparisons.\n",
        "- Modeling (Store 1): compared a temporal linear model vs RandomForest (features: day index, CPI,\n",
        "  Unemployment, Fuel_Price, Temperature, Holiday_Flag, cyclical month). The script reports RMSE/MAE.\n",
        "  RandomForest often captures non-linearity and interactions; LinearRegression is interpretable (coeffs printed).\n",
        "\"\"\".format(store_with_max_sales, store_with_max_std)\n",
        "\n",
        "with open(\"insights_summary.txt\",\"w\") as f:\n",
        "    f.write(insights)\n",
        "\n",
        "print(\"\\nWrote automated insights to insights_summary.txt\")\n",
        "\n",
        "# ---- 5. Recommendations & next steps ----\n",
        "recommendations = \"\"\"\n",
        "Recommended next steps (for better accuracy and business value):\n",
        "1) Add lag features and rolling means (e.g., sales lag-1, lag-2, 4-week rolling mean) to capture temporal autocorrelation.\n",
        "2) Use time-series-specific models: SARIMAX (with exogenous variables), FB Prophet (now called 'prophet'),\n",
        "   or gradient boosting (XGBoost/LightGBM) with time features + lags.\n",
        "3) Engineer markdown/promotion features: the dataset lacks explicit markdown amounts — try to infer markdown events\n",
        "   from sudden sales uplift patterns or introduce retailer markup calendars if available.\n",
        "4) Cross-validate using rolling (time-series) CV instead of a single holdout; tune hyperparameters for tree-based models.\n",
        "5) Evaluate business metric: weight holiday-week errors more (per problem statement) — e.g., multiply errors on\n",
        "   the four major holiday weeks by 5 to reflect competition scoring.\n",
        "6) For supply chain decisions, produce prediction intervals (quantiles) instead of point estimates.\n",
        "\"\"\"\n",
        "\n",
        "with open(\"next_steps.txt\",\"w\") as f:\n",
        "    f.write(recommendations)\n",
        "\n",
        "print(\"\\nWrote recommended next steps to next_steps.txt\")\n",
        "'''\n",
        "\n",
        "    # Replace problematic character for latin-1 encoding for code content as well.\n",
        "    code_content = code_content.replace('—', '--')\n",
        "    pdf.multi_cell(0, 4, code_content)\n",
        "    pdf.ln(10)\n",
        "\n",
        "    pdf.output(output_filename)\n",
        "    print(f\"Report saved as {output_filename}\")\n",
        "\n",
        "create_report_pdf()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Report saved as retail_analysis_report.pdf\n"
          ]
        }
      ]
    }
  ]
}